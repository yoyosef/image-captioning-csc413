{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "from data import initialize_loader, Flickr8k\n",
    "from encoder_decoder import ResNetEncoder, Decoder\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttrDict()\n",
    "# You can play with the hyperparameters here, but to finish the assignment,\n",
    "# there is no need to tune the hyperparameters here.\n",
    "args_dict = {\n",
    "    \"learn_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 5,\n",
    "    \"log_step\": 10,\n",
    "    \"save_step\": 70,\n",
    "    \"model_path\": \"models/\",\n",
    "    \"load_model\": False,\n",
    "    \"encoder_path\": \"models/encoder-1-280.ckpt\",\n",
    "    \"decoder_path\": \"models/decoder-1-280.ckpt\",\n",
    "}\n",
    "args.update(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pkl\", 'rb') as f:\n",
    "        vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ResNetEncoder(256)\n",
    "d = Decoder(len(vocab), 256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch [0/5], Step [0/291], Loss: 9.0498\n",
      "Epoch [0/5], Step [10/291], Loss: 6.5164\n",
      "Epoch [0/5], Step [20/291], Loss: 5.2866\n",
      "Epoch [0/5], Step [30/291], Loss: 4.9057\n",
      "Epoch [0/5], Step [40/291], Loss: 4.6415\n",
      "Epoch [0/5], Step [50/291], Loss: 4.5918\n",
      "Epoch [0/5], Step [60/291], Loss: 4.5181\n",
      "Epoch [0/5], Step [70/291], Loss: 4.3418\n",
      "Epoch [0/5], Step [80/291], Loss: 4.2304\n",
      "Epoch [0/5], Step [90/291], Loss: 4.0713\n",
      "Epoch [0/5], Step [100/291], Loss: 3.9409\n",
      "Epoch [0/5], Step [110/291], Loss: 3.9829\n",
      "Epoch [0/5], Step [120/291], Loss: 3.9840\n",
      "Epoch [0/5], Step [130/291], Loss: 3.8433\n",
      "Epoch [0/5], Step [140/291], Loss: 3.8157\n",
      "Epoch [0/5], Step [150/291], Loss: 3.7090\n",
      "Epoch [0/5], Step [160/291], Loss: 3.5718\n",
      "Epoch [0/5], Step [170/291], Loss: 3.6114\n",
      "Epoch [0/5], Step [180/291], Loss: 3.7314\n",
      "Epoch [0/5], Step [190/291], Loss: 3.5437\n",
      "Epoch [0/5], Step [200/291], Loss: 3.4492\n",
      "Epoch [0/5], Step [210/291], Loss: 3.4919\n",
      "Epoch [0/5], Step [220/291], Loss: 3.2592\n",
      "Epoch [0/5], Step [230/291], Loss: 3.4414\n",
      "Epoch [0/5], Step [240/291], Loss: 3.3451\n",
      "Epoch [0/5], Step [250/291], Loss: 3.3339\n",
      "Epoch [0/5], Step [260/291], Loss: 3.3317\n",
      "Epoch [0/5], Step [270/291], Loss: 3.2989\n",
      "Epoch [0/5], Step [280/291], Loss: 3.2662\n",
      "Epoch [0/5], Step [290/291], Loss: 3.1549\n",
      "Epoch [1/5], Step [0/291], Loss: 3.1926\n",
      "Epoch [1/5], Step [10/291], Loss: 3.2559\n",
      "Epoch [1/5], Step [20/291], Loss: 3.1617\n",
      "Epoch [1/5], Step [30/291], Loss: 3.1231\n",
      "Epoch [1/5], Step [40/291], Loss: 3.0758\n",
      "Epoch [1/5], Step [50/291], Loss: 3.2380\n",
      "Epoch [1/5], Step [60/291], Loss: 3.2560\n",
      "Epoch [1/5], Step [70/291], Loss: 3.1935\n",
      "Epoch [1/5], Step [80/291], Loss: 3.1602\n",
      "Epoch [1/5], Step [90/291], Loss: 3.0609\n",
      "Epoch [1/5], Step [100/291], Loss: 3.0050\n",
      "Epoch [1/5], Step [110/291], Loss: 3.0665\n",
      "Epoch [1/5], Step [120/291], Loss: 3.1700\n",
      "Epoch [1/5], Step [130/291], Loss: 3.1033\n",
      "Epoch [1/5], Step [140/291], Loss: 3.1296\n",
      "Epoch [1/5], Step [150/291], Loss: 3.0122\n",
      "Epoch [1/5], Step [160/291], Loss: 2.9536\n",
      "Epoch [1/5], Step [170/291], Loss: 2.9780\n",
      "Epoch [1/5], Step [180/291], Loss: 3.1312\n",
      "Epoch [1/5], Step [190/291], Loss: 3.0224\n",
      "Epoch [1/5], Step [200/291], Loss: 2.9261\n",
      "Epoch [1/5], Step [210/291], Loss: 2.9338\n",
      "Epoch [1/5], Step [220/291], Loss: 2.7890\n",
      "Epoch [1/5], Step [230/291], Loss: 2.9823\n",
      "Epoch [1/5], Step [240/291], Loss: 2.9154\n",
      "Epoch [1/5], Step [250/291], Loss: 2.8909\n",
      "Epoch [1/5], Step [260/291], Loss: 2.8910\n",
      "Epoch [1/5], Step [270/291], Loss: 2.8531\n",
      "Epoch [1/5], Step [280/291], Loss: 2.8686\n",
      "Epoch [1/5], Step [290/291], Loss: 2.7334\n",
      "Epoch [2/5], Step [0/291], Loss: 2.7980\n",
      "Epoch [2/5], Step [10/291], Loss: 2.8993\n",
      "Epoch [2/5], Step [20/291], Loss: 2.8158\n",
      "Epoch [2/5], Step [30/291], Loss: 2.7462\n",
      "Epoch [2/5], Step [40/291], Loss: 2.7178\n",
      "Epoch [2/5], Step [50/291], Loss: 2.8801\n",
      "Epoch [2/5], Step [60/291], Loss: 2.8916\n",
      "Epoch [2/5], Step [70/291], Loss: 2.8463\n",
      "Epoch [2/5], Step [80/291], Loss: 2.8210\n",
      "Epoch [2/5], Step [90/291], Loss: 2.7478\n",
      "Epoch [2/5], Step [100/291], Loss: 2.7243\n",
      "Epoch [2/5], Step [110/291], Loss: 2.7564\n",
      "Epoch [2/5], Step [120/291], Loss: 2.8467\n",
      "Epoch [2/5], Step [130/291], Loss: 2.8084\n",
      "Epoch [2/5], Step [140/291], Loss: 2.8714\n",
      "Epoch [2/5], Step [150/291], Loss: 2.7216\n",
      "Epoch [2/5], Step [160/291], Loss: 2.6907\n",
      "Epoch [2/5], Step [170/291], Loss: 2.6903\n",
      "Epoch [2/5], Step [180/291], Loss: 2.8406\n",
      "Epoch [2/5], Step [190/291], Loss: 2.7763\n",
      "Epoch [2/5], Step [200/291], Loss: 2.6804\n",
      "Epoch [2/5], Step [210/291], Loss: 2.6461\n",
      "Epoch [2/5], Step [220/291], Loss: 2.5621\n",
      "Epoch [2/5], Step [230/291], Loss: 2.7295\n",
      "Epoch [2/5], Step [240/291], Loss: 2.6790\n",
      "Epoch [2/5], Step [250/291], Loss: 2.6545\n",
      "Epoch [2/5], Step [260/291], Loss: 2.6488\n",
      "Epoch [2/5], Step [270/291], Loss: 2.6065\n",
      "Epoch [2/5], Step [280/291], Loss: 2.6430\n",
      "Epoch [2/5], Step [290/291], Loss: 2.4877\n",
      "Epoch [3/5], Step [0/291], Loss: 2.5678\n",
      "Epoch [3/5], Step [10/291], Loss: 2.6709\n",
      "Epoch [3/5], Step [20/291], Loss: 2.5984\n",
      "Epoch [3/5], Step [30/291], Loss: 2.5523\n",
      "Epoch [3/5], Step [40/291], Loss: 2.4973\n",
      "Epoch [3/5], Step [50/291], Loss: 2.6654\n",
      "Epoch [3/5], Step [60/291], Loss: 2.6639\n",
      "Epoch [3/5], Step [70/291], Loss: 2.6410\n",
      "Epoch [3/5], Step [80/291], Loss: 2.6267\n",
      "Epoch [3/5], Step [90/291], Loss: 2.5537\n",
      "Epoch [3/5], Step [100/291], Loss: 2.5333\n",
      "Epoch [3/5], Step [110/291], Loss: 2.5693\n",
      "Epoch [3/5], Step [120/291], Loss: 2.6444\n",
      "Epoch [3/5], Step [130/291], Loss: 2.6055\n",
      "Epoch [3/5], Step [140/291], Loss: 2.6770\n",
      "Epoch [3/5], Step [150/291], Loss: 2.5387\n",
      "Epoch [3/5], Step [160/291], Loss: 2.5158\n",
      "Epoch [3/5], Step [170/291], Loss: 2.5004\n",
      "Epoch [3/5], Step [180/291], Loss: 2.6559\n",
      "Epoch [3/5], Step [190/291], Loss: 2.6207\n",
      "Epoch [3/5], Step [200/291], Loss: 2.4920\n",
      "Epoch [3/5], Step [210/291], Loss: 2.4747\n",
      "Epoch [3/5], Step [220/291], Loss: 2.4237\n",
      "Epoch [3/5], Step [230/291], Loss: 2.5762\n",
      "Epoch [3/5], Step [240/291], Loss: 2.4940\n",
      "Epoch [3/5], Step [250/291], Loss: 2.4860\n",
      "Epoch [3/5], Step [260/291], Loss: 2.4859\n",
      "Epoch [3/5], Step [270/291], Loss: 2.4508\n",
      "Epoch [3/5], Step [280/291], Loss: 2.4820\n",
      "Epoch [3/5], Step [290/291], Loss: 2.3177\n",
      "Epoch [4/5], Step [0/291], Loss: 2.4199\n",
      "Epoch [4/5], Step [10/291], Loss: 2.5298\n",
      "Epoch [4/5], Step [20/291], Loss: 2.4708\n",
      "Epoch [4/5], Step [30/291], Loss: 2.4098\n",
      "Epoch [4/5], Step [40/291], Loss: 2.3540\n",
      "Epoch [4/5], Step [50/291], Loss: 2.5125\n",
      "Epoch [4/5], Step [60/291], Loss: 2.5046\n",
      "Epoch [4/5], Step [70/291], Loss: 2.4928\n",
      "Epoch [4/5], Step [80/291], Loss: 2.4737\n",
      "Epoch [4/5], Step [90/291], Loss: 2.4172\n",
      "Epoch [4/5], Step [100/291], Loss: 2.4022\n",
      "Epoch [4/5], Step [110/291], Loss: 2.4188\n",
      "Epoch [4/5], Step [120/291], Loss: 2.4982\n",
      "Epoch [4/5], Step [130/291], Loss: 2.4728\n",
      "Epoch [4/5], Step [140/291], Loss: 2.5406\n",
      "Epoch [4/5], Step [150/291], Loss: 2.3946\n",
      "Epoch [4/5], Step [160/291], Loss: 2.3743\n",
      "Epoch [4/5], Step [170/291], Loss: 2.3577\n",
      "Epoch [4/5], Step [180/291], Loss: 2.5066\n",
      "Epoch [4/5], Step [190/291], Loss: 2.4706\n",
      "Epoch [4/5], Step [200/291], Loss: 2.3416\n",
      "Epoch [4/5], Step [210/291], Loss: 2.3335\n",
      "Epoch [4/5], Step [220/291], Loss: 2.3066\n",
      "Epoch [4/5], Step [230/291], Loss: 2.4487\n",
      "Epoch [4/5], Step [240/291], Loss: 2.3893\n",
      "Epoch [4/5], Step [250/291], Loss: 2.3391\n",
      "Epoch [4/5], Step [260/291], Loss: 2.3589\n",
      "Epoch [4/5], Step [270/291], Loss: 2.3214\n",
      "Epoch [4/5], Step [280/291], Loss: 2.3566\n",
      "Epoch [4/5], Step [290/291], Loss: 2.2037\n"
     ]
    }
   ],
   "source": [
    "train(e, d, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "train_data = Flickr8k(csv_file=\"flickr8k/train.csv\", root_dir=\"flickr8k/images\", vocab=vocab, transform=transform)\n",
    "train_loader = initialize_loader(train_data, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d25d795bb3a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult_caption\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def caption_image(encoderCNN, decoderRNN, image, vocabulary, max_length=50):\n",
    "    # FROM https://github.com/aladdinpersson/Machine-Learning-Collection/blob/4bd862577ae445852da1c1603ade344d3eb03679/ML/Pytorch/more_advanced/image_captioning/model.py#L49\n",
    "    # NEED TO CHECK IF IT MAKES SENSE\n",
    "    result_caption = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = encoderCNN(image).unsqueeze(0)\n",
    "        states = None\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            hiddens, states = decoderRNN.lstm(x, states)\n",
    "            output = decoderRNN.linear(hiddens.squeeze(0))\n",
    "            predicted = output.argmax(1)\n",
    "            result_caption.append(predicted.item())\n",
    "            x = decoderRNN.embedding(predicted).unsqueeze(0)\n",
    "\n",
    "            if vocabulary.itos[predicted.item()] == \"<eos>\":\n",
    "                break\n",
    "\n",
    "    return [vocabulary.itos[idx] for idx in result_caption]\n",
    "\n",
    "for i, (imgs, captions, lengths) in enumerate(train_loader):\n",
    "    imgs = imgs.to(device)\n",
    "    captions = captions.to(device)\n",
    "    r = random.randint(0,64)\n",
    "    img = imgs[r].unsqueeze(0).to(device)\n",
    "    e.eval()\n",
    "    d.eval()\n",
    "    print(\" \".join(caption_image(e, d, imgs[r].unsqueeze(0), vocab)))\n",
    "    plt.imshow(imgs[r].permute(1, 2, 0).cpu())\n",
    "    sentence = map(lambda x: train_data.vocab.itos[x], captions[r])\n",
    "    print(\" \".join(sentence))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
