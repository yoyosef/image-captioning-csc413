{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "from data import initialize_loader, Flickr8k\n",
    "from encoder_decoder import ResNetEncoder, Decoder, DecoderWithAttention, ResNetAttentionEncoder\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from utils import *\n",
    "from validation import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttrDict()\n",
    "# You can play with the hyperparameters here, but to finish the assignment,\n",
    "# there is no need to tune the hyperparameters here.\n",
    "args_dict = {\n",
    "    \"embed_size\": 256,\n",
    "    \"hidden_size\": 512,\n",
    "    \"encoder_dim\": 512, # MUST MATCH THE RESNET ENCODER OUTPUT\n",
    "    \"attention_dim\": 128,\n",
    "    \"learn_rate\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"log_step\": 25,\n",
    "    \"save_epoch\": 1,\n",
    "    \"model_path\": \"models/\",\n",
    "    \"load_model\": False,\n",
    "    \"encoder_path\": \"models/encoder-attention-7.ckpt\",\n",
    "    \"decoder_path\": \"models/decoder-attention-7.ckpt\",\n",
    "    \"model_type\": \"attention\",\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"early_stopping_metric\": \"bleu\",\n",
    "    \"finetune_attention\": True\n",
    "}\n",
    "args.update(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n",
      "Epoch [1/10], Step [0/1012], Loss: 9.0763\n",
      "Epoch [1/10], Step [25/1012], Loss: 4.9860\n",
      "Epoch [1/10], Step [50/1012], Loss: 4.4838\n",
      "Epoch [1/10], Step [75/1012], Loss: 4.3801\n",
      "Epoch [1/10], Step [100/1012], Loss: 4.0152\n",
      "Epoch [1/10], Step [125/1012], Loss: 3.9713\n",
      "Epoch [1/10], Step [150/1012], Loss: 4.1160\n",
      "Epoch [1/10], Step [175/1012], Loss: 3.6588\n",
      "Epoch [1/10], Step [200/1012], Loss: 3.4512\n",
      "Epoch [1/10], Step [225/1012], Loss: 3.5083\n",
      "Epoch [1/10], Step [250/1012], Loss: 3.3980\n",
      "Epoch [1/10], Step [275/1012], Loss: 3.5858\n",
      "Epoch [1/10], Step [300/1012], Loss: 3.6223\n",
      "Epoch [1/10], Step [325/1012], Loss: 3.5001\n",
      "Epoch [1/10], Step [350/1012], Loss: 3.5167\n",
      "Epoch [1/10], Step [375/1012], Loss: 3.3607\n",
      "Epoch [1/10], Step [400/1012], Loss: 3.6197\n",
      "Epoch [1/10], Step [425/1012], Loss: 3.1537\n",
      "Epoch [1/10], Step [450/1012], Loss: 3.4472\n",
      "Epoch [1/10], Step [475/1012], Loss: 3.1947\n",
      "Epoch [1/10], Step [500/1012], Loss: 3.2218\n",
      "Epoch [1/10], Step [525/1012], Loss: 3.3452\n",
      "Epoch [1/10], Step [550/1012], Loss: 3.3003\n",
      "Epoch [1/10], Step [575/1012], Loss: 3.2956\n",
      "Epoch [1/10], Step [600/1012], Loss: 3.1143\n",
      "Epoch [1/10], Step [625/1012], Loss: 3.4381\n",
      "Epoch [1/10], Step [650/1012], Loss: 2.8808\n",
      "Epoch [1/10], Step [675/1012], Loss: 3.2085\n",
      "Epoch [1/10], Step [700/1012], Loss: 3.1895\n",
      "Epoch [1/10], Step [725/1012], Loss: 3.0382\n",
      "Epoch [1/10], Step [750/1012], Loss: 2.8285\n",
      "Epoch [1/10], Step [775/1012], Loss: 3.0246\n",
      "Epoch [1/10], Step [800/1012], Loss: 3.0806\n",
      "Epoch [1/10], Step [825/1012], Loss: 3.2887\n",
      "Epoch [1/10], Step [850/1012], Loss: 3.3086\n",
      "Epoch [1/10], Step [875/1012], Loss: 2.9465\n",
      "Epoch [1/10], Step [900/1012], Loss: 2.7873\n",
      "Epoch [1/10], Step [925/1012], Loss: 3.1010\n",
      "Epoch [1/10], Step [950/1012], Loss: 2.9048\n",
      "Epoch [1/10], Step [975/1012], Loss: 2.8516\n",
      "Epoch [1/10], Step [1000/1012], Loss: 2.8583\n",
      "Epoch [1/10], Bleu Score: 0.5618639588356018\n",
      "Epoch [1/10], Loss: 3.4974, Time (s): 234\n",
      "Epoch [2/10], Step [0/1012], Loss: 2.7226\n",
      "Epoch [2/10], Step [25/1012], Loss: 2.8998\n",
      "Epoch [2/10], Step [50/1012], Loss: 2.9738\n",
      "Epoch [2/10], Step [75/1012], Loss: 2.7782\n",
      "Epoch [2/10], Step [100/1012], Loss: 3.1128\n",
      "Epoch [2/10], Step [125/1012], Loss: 2.8838\n",
      "Epoch [2/10], Step [150/1012], Loss: 2.7731\n",
      "Epoch [2/10], Step [175/1012], Loss: 2.5691\n",
      "Epoch [2/10], Step [200/1012], Loss: 2.6578\n",
      "Epoch [2/10], Step [225/1012], Loss: 3.1903\n",
      "Epoch [2/10], Step [250/1012], Loss: 3.0113\n",
      "Epoch [2/10], Step [275/1012], Loss: 2.9384\n",
      "Epoch [2/10], Step [300/1012], Loss: 2.8207\n",
      "Epoch [2/10], Step [325/1012], Loss: 3.1125\n",
      "Epoch [2/10], Step [350/1012], Loss: 2.6240\n",
      "Epoch [2/10], Step [375/1012], Loss: 3.0059\n",
      "Epoch [2/10], Step [400/1012], Loss: 2.8105\n",
      "Epoch [2/10], Step [425/1012], Loss: 2.4614\n",
      "Epoch [2/10], Step [450/1012], Loss: 3.0042\n",
      "Epoch [2/10], Step [475/1012], Loss: 2.8045\n",
      "Epoch [2/10], Step [500/1012], Loss: 2.8073\n",
      "Epoch [2/10], Step [525/1012], Loss: 2.7144\n",
      "Epoch [2/10], Step [550/1012], Loss: 2.8637\n",
      "Epoch [2/10], Step [575/1012], Loss: 2.7001\n",
      "Epoch [2/10], Step [600/1012], Loss: 2.7426\n",
      "Epoch [2/10], Step [625/1012], Loss: 2.8216\n",
      "Epoch [2/10], Step [650/1012], Loss: 2.9417\n",
      "Epoch [2/10], Step [675/1012], Loss: 2.6127\n",
      "Epoch [2/10], Step [700/1012], Loss: 2.8305\n",
      "Epoch [2/10], Step [725/1012], Loss: 2.7674\n",
      "Epoch [2/10], Step [750/1012], Loss: 2.6745\n",
      "Epoch [2/10], Step [775/1012], Loss: 2.5208\n",
      "Epoch [2/10], Step [800/1012], Loss: 2.6447\n",
      "Epoch [2/10], Step [825/1012], Loss: 2.7511\n",
      "Epoch [2/10], Step [850/1012], Loss: 2.9802\n",
      "Epoch [2/10], Step [875/1012], Loss: 2.6052\n",
      "Epoch [2/10], Step [900/1012], Loss: 2.6818\n",
      "Epoch [2/10], Step [925/1012], Loss: 2.5743\n",
      "Epoch [2/10], Step [950/1012], Loss: 2.6735\n",
      "Epoch [2/10], Step [975/1012], Loss: 2.6097\n",
      "Epoch [2/10], Step [1000/1012], Loss: 2.5661\n",
      "Epoch [2/10], Bleu Score: 0.6012954115867615\n",
      "Epoch [2/10], Loss: 2.7180, Time (s): 462\n",
      "Epoch [3/10], Step [0/1012], Loss: 2.3151\n",
      "Epoch [3/10], Step [25/1012], Loss: 2.4651\n",
      "Epoch [3/10], Step [50/1012], Loss: 2.5057\n",
      "Epoch [3/10], Step [75/1012], Loss: 2.2318\n",
      "Epoch [3/10], Step [100/1012], Loss: 2.2462\n",
      "Epoch [3/10], Step [125/1012], Loss: 2.7814\n",
      "Epoch [3/10], Step [150/1012], Loss: 2.2915\n",
      "Epoch [3/10], Step [175/1012], Loss: 2.4157\n",
      "Epoch [3/10], Step [200/1012], Loss: 2.5077\n",
      "Epoch [3/10], Step [225/1012], Loss: 2.3141\n",
      "Epoch [3/10], Step [250/1012], Loss: 2.3195\n",
      "Epoch [3/10], Step [275/1012], Loss: 2.3339\n",
      "Epoch [3/10], Step [300/1012], Loss: 2.1994\n",
      "Epoch [3/10], Step [325/1012], Loss: 2.4008\n",
      "Epoch [3/10], Step [350/1012], Loss: 2.5662\n",
      "Epoch [3/10], Step [375/1012], Loss: 2.5138\n",
      "Epoch [3/10], Step [400/1012], Loss: 2.2752\n",
      "Epoch [3/10], Step [425/1012], Loss: 2.3926\n",
      "Epoch [3/10], Step [450/1012], Loss: 2.2517\n",
      "Epoch [3/10], Step [475/1012], Loss: 2.3090\n",
      "Epoch [3/10], Step [500/1012], Loss: 2.0747\n",
      "Epoch [3/10], Step [525/1012], Loss: 2.1558\n",
      "Epoch [3/10], Step [550/1012], Loss: 2.2328\n",
      "Epoch [3/10], Step [575/1012], Loss: 2.5160\n",
      "Epoch [3/10], Step [600/1012], Loss: 2.6462\n",
      "Epoch [3/10], Step [625/1012], Loss: 2.3687\n",
      "Epoch [3/10], Step [650/1012], Loss: 2.4798\n",
      "Epoch [3/10], Step [675/1012], Loss: 2.3505\n",
      "Epoch [3/10], Step [700/1012], Loss: 2.4452\n",
      "Epoch [3/10], Step [725/1012], Loss: 2.3765\n",
      "Epoch [3/10], Step [750/1012], Loss: 2.4568\n",
      "Epoch [3/10], Step [775/1012], Loss: 2.4620\n",
      "Epoch [3/10], Step [800/1012], Loss: 2.3568\n",
      "Epoch [3/10], Step [825/1012], Loss: 2.3398\n",
      "Epoch [3/10], Step [850/1012], Loss: 2.5761\n",
      "Epoch [3/10], Step [875/1012], Loss: 2.3849\n",
      "Epoch [3/10], Step [900/1012], Loss: 2.2939\n",
      "Epoch [3/10], Step [925/1012], Loss: 2.5089\n",
      "Epoch [3/10], Step [950/1012], Loss: 2.3169\n",
      "Epoch [3/10], Step [975/1012], Loss: 2.2833\n",
      "Epoch [3/10], Step [1000/1012], Loss: 2.4520\n",
      "Epoch [3/10], Bleu Score: 0.5928220748901367\n",
      "Epoch [3/10], Loss: 2.3879, Time (s): 702\n",
      "Bleu score has not improved in 1 epochs, stopping early\n",
      "Obtained highest bleu score of: 0.6012954115867615\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.pkl\", 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "train_data = Flickr8k(csv_file=\"flickr8k/train.csv\", root_dir=\"flickr8k/images\", vocab=vocab, transform=transform)\n",
    "train_loader = initialize_loader(train_data, batch_size=args.batch_size)\n",
    "val_data = Flickr8k(csv_file=\"flickr8k/val.csv\",\n",
    "                        root_dir=\"flickr8k/images\", vocab=vocab, transform=transform)\n",
    "val_loader = initialize_loader(val_data, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/encoder-attention-7.ckpt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-61d8707ae63b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/encoder-attention-7.ckpt'"
     ]
    }
   ],
   "source": [
    "if args.model_type == \"attention\":\n",
    "    e = ResNetAttentionEncoder(args.embed_size)\n",
    "    d = DecoderWithAttention(len(\n",
    "        vocab), args.embed_size, args.hidden_size, args.encoder_dim, args.attention_dim)\n",
    "else:\n",
    "    e = ResNetEncoder(args.embed_size)\n",
    "    d = Decoder(len(vocab), args.embed_size, args.hidden_size)\n",
    "\n",
    "e.load_state_dict(torch.load(args.encoder_path))\n",
    "d.load_state_dict(torch.load(args.decoder_path))\n",
    "\n",
    "e.to(device)\n",
    "d.to(device)\n",
    "\n",
    "rand_num = random.randint(0, 1000)\n",
    "img = val_data[rand_num][0]\n",
    "if args.model_type == \"attention\":\n",
    "    caps, alphas = get_caption_attention(e, d, img, vocab)\n",
    "    plot_attention(img, caps, alphas)\n",
    "else:\n",
    "    caps = get_caption_lstm(e, d, img, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6074705719947815"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "validation_bleu1(e, d, vocab, val_data, attention=(True if args.model_type == \"attention\" else False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python391jvsc74a57bd0e256ddef18e657785f20b71a449ae5cdeb0aad26c6e24ee4ccc4abb29ad37536",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}